{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a83af7-48b3-495b-af30-3bf2d0fcb169",
   "metadata": {},
   "source": [
    "ğŸŒ Turning Data Cleaning into a Website (Simple Guide)\n",
    "| Step                                       | What It Means (Non-Technical)                                                          | What Happens Behind the Scenes                                                                                                 |\n",
    "| ------------------------------------------ | -------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **1. Plan the Website**                    | Decide what the site will do: people upload data, it cleans it, they download results. | We define the features: upload â†’ clean â†’ download.                                                                             |\n",
    "| **2. Build the Frontend (User Side)**      | This is the part users see: the website page with buttons, forms, and progress bars.   | Tools like React/Next.js are used to design pages: Upload button, Recipe builder (choose cleaning options), and Download link. |\n",
    "| **3. Build the Backend (Engine Room)**     | This is the hidden part that does the heavy lifting â€” the actual cleaning of data.     | A system (e.g., FastAPI in Python) receives requests from the website, starts cleaning jobs, and returns results.              |\n",
    "| **4. Add Storage (Warehouse for Files)**   | Uploaded files need to be stored safely before cleaning.                               | Files are sent to a cloud â€œbucketâ€ (like Dropbox but for computers). Examples: Amazon S3, Google Cloud Storage.                |\n",
    "| **5. Add Workers (Cleaning Staff)**        | Imagine workers in the warehouse who open the box, clean the data, and repackage it.   | Special programs (Celery workers) run the Python cleaning code (your DataCleaner class).                                       |\n",
    "| **6. Create Jobs (Tickets to Track Work)** | Every time someone uploads data, a â€œticketâ€ is created to track progress.              | The system logs: job started â†’ cleaning â†’ done â†’ ready to download.                                                            |\n",
    "| **7. Reports & Results**                   | Users donâ€™t just want cleaned data, they want to see what changed.                     | A report is created (e.g., rows removed, duplicates fixed, missing values filled) and stored for download.                     |\n",
    "| **8. Security & Accounts**                 | Not everyone should see everyoneâ€™s data.                                               | The system requires login (email + password or Google login) and keeps data separated per company/user.                        |\n",
    "| **9. Test Everything**                     | Try uploading messy data to see if the website cleans it correctly.                    | Run test files through the system, fix errors, make sure progress bar and downloads work.                                      |\n",
    "| **10. Deploy (Put It Online)**             | Make the site public so people around the world can use it.                            | Upload the website and backend to cloud servers (AWS, Google Cloud, Azure, etc.). Add a domain name (like `datacleaner.com`).  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b3f64-4e5f-4005-af03-579942041929",
   "metadata": {},
   "source": [
    "# 1) Define scope & requirements\n",
    "\n",
    "**Core use-cases**\n",
    "\n",
    "* Users upload datasets (CSV, Excel, JSON, Parquet) or connect a data source (S3, GDrive, DB).\n",
    "* Users configure a **cleaning recipe** (missing data strategy, dtypes, outliers, encoding, scaling).\n",
    "* System runs cleaning asynchronously, tracks progress, and lets users download results + reports.\n",
    "* Reproducibility: save versions of datasets + recipes + logs + validation reports.\n",
    "\n",
    "**Non-functionals**\n",
    "\n",
    "* Multi-tenant (orgs/teams), secure, GDPR-friendly, scalable.\n",
    "* Works with large files (GBs) via chunked/multipart uploads and background processing.\n",
    "* Observability, audit logs, retries, idempotency.\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Architecture (high level)\n",
    "\n",
    "**Frontend (SPA/SSR):**\n",
    "\n",
    "* React or Next.js (TypeScript), Tailwind/Chakra UI, React Query.\n",
    "* Drag-and-drop upload, recipe builder UI, live job status, dataset preview, diff & quality reports.\n",
    "\n",
    "**Backend API:**\n",
    "\n",
    "* Python **FastAPI** (typed, fast, good OpenAPI).\n",
    "* Auth: JWT for API, session cookies for web. Optional SSO (OAuth2/OIDC).\n",
    "* Endpoints for datasets, recipes, jobs, reports, downloads, connectors.\n",
    "\n",
    "**Async Processing:**\n",
    "\n",
    "* Celery/RQ workers with Redis as broker & result backend (or Redis + Postgres for results).\n",
    "* Workers execute cleaning pipeline on uploaded files.\n",
    "* For big data: scale workers, optionally Dask/Spark for > memory.\n",
    "\n",
    "**Storage:**\n",
    "\n",
    "* Object storage for files: S3 (prod), MinIO (dev).\n",
    "* Postgres for metadata (users, orgs, datasets, jobs, recipes).\n",
    "* Redis for queues/cache.\n",
    "* CDN for downloads.\n",
    "\n",
    "**Infra/DevOps:**\n",
    "\n",
    "* Docker + docker-compose (dev), Kubernetes (prod), IaC (Terraform).\n",
    "* CI/CD (GitHub Actions): tests, build, push images, run migrations, blue/green deploy.\n",
    "\n",
    "**Observability/Security:**\n",
    "\n",
    "* Logging (structured JSON), Sentry for errors, Prometheus + Grafana for metrics.\n",
    "* VPC, private subnets for workers/DB, KMS-managed encryption, WAF/CDN, secrets manager.\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Tech stack (recommended)\n",
    "\n",
    "* **Frontend**: Next.js 14, TypeScript, Tailwind, React Query, Zod, React Hook Form, i18n.\n",
    "* **Backend**: FastAPI, Pydantic v2, SQLAlchemy 2.x, Alembic, Uvicorn.\n",
    "* **Async**: Celery + Redis (or RQ), Flower or Arq dashboard.\n",
    "* **Data**: Pandas, PyArrow, Pandera/Great Expectations (validation), Dask (optional), openpyxl.\n",
    "* **Storage**: Postgres, S3/MinIO, Redis.\n",
    "* **Auth**: JWT (PyJWT), OAuthlib/`authlib` for OIDC.\n",
    "* **Testing**: Pytest, Playwright (e2e), Locust (load), Bandit/Semgrep (security).\n",
    "* **CI/CD**: GitHub Actions, Docker, Terraform, Helm.\n",
    "\n",
    "---\n",
    "\n",
    "# 4) Data model (core tables)\n",
    "\n",
    "**organizations**\n",
    "\n",
    "* id (uuid), name, plan, created\\_at\n",
    "\n",
    "**users**\n",
    "\n",
    "* id (uuid), org\\_id (fk), email, password\\_hash (or external\\_id for SSO), role, created\\_at\n",
    "\n",
    "**datasets**\n",
    "\n",
    "* id (uuid), org\\_id (fk), owner\\_id (fk), name, original\\_filename, file\\_type, size\\_bytes\n",
    "* storage\\_key (S3 path), schema\\_json (inferred), row\\_count, status \\[uploaded|validated|processed|failed]\n",
    "* created\\_at, deleted\\_at\n",
    "\n",
    "**recipes**\n",
    "\n",
    "* id (uuid), org\\_id, name, version (int), active (bool)\n",
    "* config\\_json (see â€œRecipe formatâ€ below), created\\_by, created\\_at\n",
    "\n",
    "**jobs**\n",
    "\n",
    "* id (uuid), org\\_id, dataset\\_id, recipe\\_id, status \\[queued|running|succeeded|failed|canceled]\n",
    "* progress (0â€“100), started\\_at, finished\\_at, worker\\_id, logs\\_url, output\\_storage\\_key\n",
    "* metrics\\_json (e.g., rows\\_dropped, nulls\\_filled, outliers\\_handled)\n",
    "\n",
    "**reports**\n",
    "\n",
    "* id (uuid), job\\_id, validation\\_summary\\_json, profile\\_html\\_key, created\\_at\n",
    "\n",
    "**audit\\_logs**\n",
    "\n",
    "* id, org\\_id, user\\_id, action, target\\_type, target\\_id, metadata\\_json, created\\_at\n",
    "\n",
    "Indexes for org\\_id, created\\_at, status. Store files in S3 keyed `org/{org_id}/dataset/{dataset_id}/original.*`.\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Recipe format (portable & versioned)\n",
    "\n",
    "Use JSON (or YAML) to describe a cleaning pipeline as ordered steps:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"default_recipe\",\n",
    "  \"version\": 3,\n",
    "  \"steps\": [\n",
    "    {\"op\": \"fix_dtypes\", \"params\": {\"date_joined\": \"datetime\", \"id\": \"string\"}},\n",
    "    {\"op\": \"handle_missing\", \"params\": {\"strategy\": \"median\", \"custom\": {\"city\": \"unknown\"}}},\n",
    "    {\"op\": \"remove_duplicates\"},\n",
    "    {\"op\": \"handle_outliers\", \"params\": {\"cols\": [\"age\", \"salary\"], \"method\": \"IQR\"}},\n",
    "    {\"op\": \"clean_text\", \"params\": {\"cols\": [\"name\", \"gender\"]}},\n",
    "    {\"op\": \"encode_categoricals\", \"params\": {\"cols\": [\"gender\"], \"method\": \"onehot\"}},\n",
    "    {\"op\": \"scale_numeric\", \"params\": {\"cols\": [\"age\", \"salary\"], \"method\": \"standard\"}}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Youâ€™ll render this into UI forms via a **JSON Schema** so users can compose recipes without code.\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Backend: key endpoints (FastAPI)\n",
    "\n",
    "### Auth\n",
    "\n",
    "* `POST /auth/signup` â€” email/password (or SSO invite flow)\n",
    "* `POST /auth/login` â€” returns JWT or sets secure cookie\n",
    "* `POST /auth/logout`\n",
    "* `GET /me`\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* `POST /datasets/upload-url` â€” returns S3 pre-signed URL for multipart upload\n",
    "* `POST /datasets/complete-multipart` â€” finalize upload; server infers schema/profile\n",
    "* `GET /datasets` â€” list (filter by status)\n",
    "* `GET /datasets/{id}` â€” metadata\n",
    "* `DELETE /datasets/{id}` â€” soft delete + lifecycle policy to purge after N days\n",
    "\n",
    "### Recipes\n",
    "\n",
    "* `GET /recipes` | `POST /recipes` | `PUT /recipes/{id}` | `POST /recipes/{id}/clone`\n",
    "\n",
    "### Jobs\n",
    "\n",
    "* `POST /jobs` â€” body: {dataset\\_id, recipe\\_id}\n",
    "* `GET /jobs` â€” list by dataset/recipe/status\n",
    "* `GET /jobs/{id}` â€” status, progress, metrics\n",
    "* `POST /jobs/{id}/cancel`\n",
    "\n",
    "### Results\n",
    "\n",
    "* `GET /jobs/{id}/download` â€” pre-signed URL (CSV/Parquet)\n",
    "* `GET /jobs/{id}/report` â€” profile/validation summary\n",
    "\n",
    "### WebSocket (optional)\n",
    "\n",
    "* `/ws/jobs/{id}` â€” real-time progress updates\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Backend implementation details\n",
    "\n",
    "## 7.1 FastAPI scaffolding & deps\n",
    "\n",
    "```bash\n",
    "pip install fastapi uvicorn[standard] python-multipart boto3 pydantic-settings sqlalchemy psycopg[binary] alembic redis celery pandas pyarrow openpyxl\n",
    "```\n",
    "\n",
    "**`app/main.py`**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from app.routers import auth, datasets, recipes, jobs\n",
    "\n",
    "app = FastAPI(title=\"Data Cleaning Platform\", version=\"1.0\")\n",
    "\n",
    "app.include_router(auth.router, prefix=\"/auth\", tags=[\"auth\"])\n",
    "app.include_router(datasets.router, prefix=\"/datasets\", tags=[\"datasets\"])\n",
    "app.include_router(recipes.router, prefix=\"/recipes\", tags=[\"recipes\"])\n",
    "app.include_router(jobs.router, prefix=\"/jobs\", tags=[\"jobs\"])\n",
    "```\n",
    "\n",
    "## 7.2 Models (SQLAlchemy 2.x)\n",
    "\n",
    "```python\n",
    "# app/models.py\n",
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship\n",
    "from sqlalchemy import String, ForeignKey, JSON, BigInteger, Enum\n",
    "import uuid, enum\n",
    "\n",
    "class Base(DeclarativeBase): pass\n",
    "\n",
    "class JobStatus(str, enum.Enum):\n",
    "    queued=\"queued\"; running=\"running\"; succeeded=\"succeeded\"; failed=\"failed\"; canceled=\"canceled\"\n",
    "\n",
    "class Dataset(Base):\n",
    "    __tablename__ = \"datasets\"\n",
    "    id: Mapped[uuid.UUID] = mapped_column(primary_key=True, default=uuid.uuid4)\n",
    "    org_id: Mapped[uuid.UUID] = mapped_column(index=True)\n",
    "    owner_id: Mapped[uuid.UUID] = mapped_column(index=True)\n",
    "    name: Mapped[str] = mapped_column(String(255))\n",
    "    original_filename: Mapped[str] = mapped_column(String(512))\n",
    "    file_type: Mapped[str] = mapped_column(String(32))\n",
    "    size_bytes: Mapped[int] = mapped_column(BigInteger)\n",
    "    storage_key: Mapped[str] = mapped_column(String(1024))\n",
    "    schema_json: Mapped[dict] = mapped_column(JSON)\n",
    "    row_count: Mapped[int | None]\n",
    "    status: Mapped[str] = mapped_column(String(32), index=True)\n",
    "\n",
    "class Recipe(Base):\n",
    "    __tablename__ = \"recipes\"\n",
    "    id: Mapped[uuid.UUID] = mapped_column(primary_key=True, default=uuid.uuid4)\n",
    "    org_id: Mapped[uuid.UUID] = mapped_column(index=True)\n",
    "    name: Mapped[str] = mapped_column(String(255))\n",
    "    version: Mapped[int]\n",
    "    config_json: Mapped[dict] = mapped_column(JSON)\n",
    "\n",
    "class Job(Base):\n",
    "    __tablename__ = \"jobs\"\n",
    "    id: Mapped[uuid.UUID] = mapped_column(primary_key=True, default=uuid.uuid4)\n",
    "    org_id: Mapped[uuid.UUID] = mapped_column(index=True)\n",
    "    dataset_id: Mapped[uuid.UUID] = mapped_column(ForeignKey(\"datasets.id\"))\n",
    "    recipe_id: Mapped[uuid.UUID] = mapped_column(ForeignKey(\"recipes.id\"))\n",
    "    status: Mapped[JobStatus] = mapped_column(Enum(JobStatus), index=True)\n",
    "    progress: Mapped[int]\n",
    "    output_storage_key: Mapped[str | None]\n",
    "    metrics_json: Mapped[dict | None]\n",
    "```\n",
    "\n",
    "Run migrations with Alembic.\n",
    "\n",
    "## 7.3 S3 uploads (pre-signed URLs)\n",
    "\n",
    "```python\n",
    "# app/routers/datasets.py\n",
    "@router.post(\"/upload-url\")\n",
    "def create_upload_url(file_name: str, file_type: str, org=Depends(auth_org)):\n",
    "    key = f\"org/{org.id}/dataset/{uuid4()}/{file_name}\"\n",
    "    url = s3_client.generate_presigned_url(\n",
    "        ClientMethod=\"put_object\",\n",
    "        Params={\"Bucket\": settings.S3_BUCKET, \"Key\": key, \"ContentType\": file_type},\n",
    "        ExpiresIn=3600)\n",
    "    return {\"upload_url\": url, \"storage_key\": key}\n",
    "```\n",
    "\n",
    "Client uploads directly to S3; then you `POST /datasets/complete-multipart` to persist metadata + kick off profiling (small read from S3 to infer schema).\n",
    "\n",
    "## 7.4 Async job execution (Celery)\n",
    "\n",
    "```python\n",
    "# app/celery_app.py\n",
    "from celery import Celery\n",
    "celery = Celery(__name__, broker=settings.REDIS_URL, backend=settings.REDIS_URL)\n",
    "\n",
    "# app/tasks.py\n",
    "@celery.task(bind=True)\n",
    "def run_cleaning_job(self, job_id: str):\n",
    "    # 1) Fetch job, dataset, recipe from DB\n",
    "    # 2) Stream file from S3 -> local tmp\n",
    "    # 3) Execute pipeline (chunked if large)\n",
    "    # 4) Write output to Parquet/CSV -> S3\n",
    "    # 5) Generate profile/report -> S3\n",
    "    # 6) Update DB: status, progress, metrics, output key\n",
    "    ...\n",
    "```\n",
    "\n",
    "Kick off from API:\n",
    "\n",
    "```python\n",
    "@router.post(\"/\")\n",
    "def create_job(req: CreateJobRequest, user=Depends(auth_user)):\n",
    "    job = Job(..., status=JobStatus.queued, progress=0)\n",
    "    db.add(job); db.commit()\n",
    "    run_cleaning_job.delay(str(job.id))\n",
    "    return {\"job_id\": job.id}\n",
    "```\n",
    "\n",
    "## 7.5 Pipeline execution (plugin-style)\n",
    "\n",
    "Create a **registry** of operations that map to functions taking (df, params) -> df. This makes recipes extensible.\n",
    "\n",
    "```python\n",
    "# app/pipeline/ops.py\n",
    "REGISTRY = {}\n",
    "\n",
    "def op(name):\n",
    "    def deco(fn):\n",
    "        REGISTRY[name] = fn\n",
    "        return fn\n",
    "    return deco\n",
    "\n",
    "@op(\"fix_dtypes\")\n",
    "def fix_dtypes(df, params):\n",
    "    for col, dtype in params.items():\n",
    "        if dtype == \"datetime\":\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        elif dtype == \"string\":\n",
    "            df[col] = df[col].astype(\"string\")\n",
    "        else:\n",
    "            df[col] = df[col].astype(dtype)\n",
    "    return df\n",
    "\n",
    "@op(\"handle_missing\")\n",
    "def handle_missing(df, params):\n",
    "    strat = params.get(\"strategy\", \"median\")\n",
    "    custom = params.get(\"custom\", {})\n",
    "    if strat == \"median\":\n",
    "        df = df.fillna(df.median(numeric_only=True))\n",
    "    elif strat == \"mean\":\n",
    "        df = df.fillna(df.mean(numeric_only=True))\n",
    "    elif strat == \"mode\":\n",
    "        for c in df.columns:\n",
    "            df[c] = df[c].fillna(df[c].mode().iloc[0])\n",
    "    if custom:\n",
    "        df = df.fillna(custom)\n",
    "    return df\n",
    "\n",
    "# ... add other ops (remove_duplicates, handle_outliers, clean_text, encode_categoricals, scale_numeric)\n",
    "```\n",
    "\n",
    "Execute:\n",
    "\n",
    "```python\n",
    "def run_recipe(df, recipe):\n",
    "    for step in recipe[\"steps\"]:\n",
    "        op_name = step[\"op\"]; params = step.get(\"params\", {})\n",
    "        df = REGISTRY[op_name](df, params)\n",
    "    return df\n",
    "```\n",
    "\n",
    "### Large files\n",
    "\n",
    "* Prefer **Parquet** internally. Use `pyarrow` for faster IO.\n",
    "* For CSV > 1â€“2GB: chunked read (`chunksize=100_000`), apply *only streaming-safe steps* (e.g., text clean, dtype casts, missing strategies) per chunk, write to a temp parquet dataset, then post-process (e.g., outliers need global stats: compute stats in a first pass; apply in a second pass). Alternatively, use **Dask** for lazy parallel out-of-core:\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "ddf = dd.read_csv(s3_url, blocksize=\"128MB\")\n",
    "# apply map_partitions for ops; compute global stats via ddf.describe().compute()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8) Validation & profiling\n",
    "\n",
    "* **Pandera** schemas or **Great Expectations** suites. Store validation results per job.\n",
    "* Produce an HTML report (pandas-profiling/ydata-profiling) and store to S3 for download.\n",
    "* Metrics saved in `jobs.metrics_json` for summary UI.\n",
    "\n",
    "---\n",
    "\n",
    "# 9) Security, privacy, and compliance\n",
    "\n",
    "* **PII detection**: optional step to flag columns likely containing PII (regex + nbdev patterns).\n",
    "* **Column masks**: allow users to mark columns as sensitive â†’ masked in logs/reports.\n",
    "* **Encryption**: TLS in transit; S3 SSE-KMS at rest; Postgres TDE (or encrypted volume).\n",
    "* **Access control**: org-scoped RBAC (owner/admin/member). Row-level filtering by org\\_id.\n",
    "* **Data retention**: per-plan policies (e.g., auto-delete files after N days). Right-to-erasure endpoints for GDPR.\n",
    "* **Secrets**: managed in cloud secrets manager; rotate keys; least privilege IAM.\n",
    "* **Rate limiting**: per org and per IP via API gateway/WAF.\n",
    "\n",
    "---\n",
    "\n",
    "# 10) Frontend UX (key flows)\n",
    "\n",
    "**1) Upload & preview**\n",
    "\n",
    "* Drag-and-drop area. Client-side sniff columns (first 200 rows) for immediate preview.\n",
    "* Show inferred types, null rates, sample rows.\n",
    "* If >50MB, use multipart; show progress per part; resume uploads.\n",
    "\n",
    "**2) Recipe builder**\n",
    "\n",
    "* Step catalog with toggleable cards.\n",
    "* Dynamic forms generated from **JSON Schemas** of each op.\n",
    "* Validations (Zod) consistent with backend Pydantic.\n",
    "\n",
    "**3) Job run & status**\n",
    "\n",
    "* â€œRun cleaningâ€ â†’ create job â†’ navigate to job detail.\n",
    "* Real-time status via WebSocket or poll every 2â€“4s.\n",
    "* Progress phases: download â†’ profile â†’ pass1 stats â†’ pass2 transform â†’ write â†’ validate.\n",
    "\n",
    "**4) Results & reports**\n",
    "\n",
    "* Download cleaned CSV/Parquet.\n",
    "* Compare before vs after: rows dropped, nulls filled, outliers removed (nice diffs).\n",
    "* Visualizations: null heatmap, outlier boxplots, type distribution.\n",
    "\n",
    "**5) Settings & Tokens**\n",
    "\n",
    "* API tokens for programmatic use.\n",
    "* Connectors (S3, GDrive, Postgres read-only).\n",
    "\n",
    "**Accessibility & i18n**\n",
    "\n",
    "* WCAG AA: focus states, keyboard nav, color contrast.\n",
    "* i18n (react-intl or next-intl), RTL support.\n",
    "\n",
    "---\n",
    "\n",
    "# 11) Example frontend pieces\n",
    "\n",
    "**Upload with pre-signed URL (Next.js, minimal example):**\n",
    "\n",
    "```ts\n",
    "async function uploadFile(file: File) {\n",
    "  const resp = await fetch('/api/datasets/upload-url', {\n",
    "    method: 'POST',\n",
    "    headers: {'Content-Type': 'application/json'},\n",
    "    body: JSON.stringify({ file_name: file.name, file_type: file.type })\n",
    "  }).then(r => r.json());\n",
    "\n",
    "  await fetch(resp.upload_url, {\n",
    "    method: 'PUT',\n",
    "    headers: {'Content-Type': file.type},\n",
    "    body: file\n",
    "  });\n",
    "\n",
    "  await fetch('/api/datasets/complete-multipart', {\n",
    "    method: 'POST',\n",
    "    headers: {'Content-Type': 'application/json'},\n",
    "    body: JSON.stringify({ storage_key: resp.storage_key, size_bytes: file.size })\n",
    "  });\n",
    "}\n",
    "```\n",
    "\n",
    "**Job status (polling):**\n",
    "\n",
    "```ts\n",
    "async function fetchJob(id: string) {\n",
    "  const r = await fetch(`/api/jobs/${id}`);\n",
    "  return r.json(); // {status, progress, metrics}\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 12) Testing strategy\n",
    "\n",
    "* **Unit tests**: pipeline ops (edge cases: weird dtypes, all-null columns, mixed types).\n",
    "* **Contract tests**: API schemas (Pydantic), negative tests (bad inputs).\n",
    "* **Integration tests**: upload â†’ job â†’ download (use MinIO & local Postgres/Redis via docker-compose).\n",
    "* **E2E**: Playwright to drive full UI flows.\n",
    "* **Load tests**: Locust simulating large uploads & multiple concurrent jobs.\n",
    "* **Security**: Bandit, Semgrep, dependency scanning, SSRF tests for connectors.\n",
    "\n",
    "---\n",
    "\n",
    "# 13) Containerization & local dev\n",
    "\n",
    "**Dockerfile (backend)**\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.11-slim\n",
    "WORKDIR /app\n",
    "COPY pyproject.toml poetry.lock ./\n",
    "RUN pip install --upgrade pip && pip install \"poetry>=1.7.0\"\n",
    "RUN poetry config virtualenvs.create false && poetry install --no-interaction --no-ansi\n",
    "COPY . .\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "**docker-compose.yml (dev)**\n",
    "\n",
    "```yaml\n",
    "version: \"3.9\"\n",
    "services:\n",
    "  api:\n",
    "    build: .\n",
    "    ports: [\"8000:8000\"]\n",
    "    env_file: .env.dev\n",
    "    depends_on: [db, redis, minio]\n",
    "  worker:\n",
    "    build: .\n",
    "    command: celery -A app.celery_app.celery worker -l info --concurrency=2\n",
    "    env_file: .env.dev\n",
    "    depends_on: [api, redis, minio, db]\n",
    "  db:\n",
    "    image: postgres:15\n",
    "    environment: { POSTGRES_PASSWORD: password, POSTGRES_DB: dataclean }\n",
    "    ports: [\"5432:5432\"]\n",
    "  redis:\n",
    "    image: redis:7\n",
    "    ports: [\"6379:6379\"]\n",
    "  minio:\n",
    "    image: minio/minio\n",
    "    command: server /data --console-address \":9001\"\n",
    "    environment: { MINIO_ROOT_USER: minio, MINIO_ROOT_PASSWORD: minio123 }\n",
    "    ports: [\"9000:9000\", \"9001:9001\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 14) CI/CD (GitHub Actions example)\n",
    "\n",
    "**.github/workflows/ci.yml**\n",
    "\n",
    "```yaml\n",
    "name: ci\n",
    "on: [push, pull_request]\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    services:\n",
    "      postgres:\n",
    "        image: postgres:15\n",
    "        env:\n",
    "          POSTGRES_PASSWORD: password\n",
    "          POSTGRES_DB: dataclean\n",
    "        ports: [\"5432:5432\"]\n",
    "      redis:\n",
    "        image: redis:7\n",
    "        ports: [\"6379:6379\"]\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - uses: actions/setup-python@v5\n",
    "        with: { python-version: \"3.11\" }\n",
    "      - run: pip install -r requirements.txt\n",
    "      - run: alembic upgrade head\n",
    "      - run: pytest -q\n",
    "  build-and-push:\n",
    "    needs: test\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - name: Build\n",
    "        run: docker build -t ghcr.io/you/dataclean-api:${{ github.sha }} .\n",
    "      - name: Push\n",
    "        run: echo \"push to registry here\"\n",
    "```\n",
    "\n",
    "Deploy via Helm to Kubernetes (ingress, HPA, secrets, configmaps). Blue/green or rolling updates.\n",
    "\n",
    "---\n",
    "\n",
    "# 15) Performance & scaling tips\n",
    "\n",
    "* Prefer **Parquet** for outputs; compress (snappy or zstd).\n",
    "* Stream processing or Dask for >memory.\n",
    "* Keep API stateless; workers autoscale horizontally.\n",
    "* Pre-compute schema/stats on upload for faster UX.\n",
    "* Use **presigned S3** everywhere to keep API out of data path.\n",
    "* Cache dataset previews (first N rows) in Redis.\n",
    "* Rate-limit & queue backpressure to protect workers.\n",
    "\n",
    "---\n",
    "\n",
    "# 16) Rollout plan (phased)\n",
    "\n",
    "1. **MVP**: Upload â†’ simple recipe (missing/duplicates/dtypes) â†’ job â†’ download. Local files only.\n",
    "2. **Add S3/MinIO** storage + presigned uploads + Redis + Celery.\n",
    "3. **Reports & validation**; recipe versioning; job metrics dashboard.\n",
    "4. **Multi-tenant auth/RBAC**, org workspaces, audit logs, retention policies.\n",
    "5. **Scalability**: Dask mode, autoscaling workers, CDN for downloads.\n",
    "6. **Connectors**: GDrive, S3 external, DB read-only.\n",
    "7. **Enterprise**: SSO, org-level KMS keys, private networking, custom retention.\n",
    "\n",
    "---\n",
    "\n",
    "# 17) Example: end-to-end happy path (code snippets)\n",
    "\n",
    "**Create job (FastAPI)**\n",
    "\n",
    "```python\n",
    "@router.post(\"/\", response_model=JobOut)\n",
    "def create_job(req: JobIn, user=Depends(auth_user), db=Depends(get_db)):\n",
    "    dataset = db.get(Dataset, req.dataset_id)\n",
    "    recipe = db.get(Recipe, req.recipe_id)\n",
    "    assert dataset.org_id == user.org_id == recipe.org_id\n",
    "\n",
    "    job = Job(org_id=user.org_id, dataset_id=dataset.id, recipe_id=recipe.id,\n",
    "              status=JobStatus.queued, progress=0)\n",
    "    db.add(job); db.commit()\n",
    "    run_cleaning_job.delay(str(job.id))\n",
    "    return job\n",
    "```\n",
    "\n",
    "**Worker task (simplified)**\n",
    "\n",
    "```python\n",
    "@celery.task(bind=True)\n",
    "def run_cleaning_job(self, job_id: str):\n",
    "    job = db_session.get(Job, job_id)\n",
    "    update(job, status=\"running\", progress=5)\n",
    "\n",
    "    # 1) Download from S3\n",
    "    tmp_in = download_from_s3(job.dataset.storage_key)\n",
    "    # 2) Read (CSV/Excel/Parquet)\n",
    "    df = read_any(tmp_in)  # implement sniffing by extension/magic\n",
    "    # 3) Execute recipe\n",
    "    df_clean = run_recipe(df, job.recipe.config_json)\n",
    "    # 4) Validate & profile\n",
    "    report_html, summary = profile(df_clean)\n",
    "    # 5) Write to S3\n",
    "    out_key = upload_parquet(df_clean, org_id=job.org_id, job_id=job.id)\n",
    "    report_key = upload_report(report_html, job.id)\n",
    "\n",
    "    metrics = compute_metrics(df, df_clean)\n",
    "    update(job, status=\"succeeded\", progress=100,\n",
    "           output_storage_key=out_key, metrics_json=metrics)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Final notes\n",
    "\n",
    "* Keep **recipes declarative** and **ops plugin-based** to grow features without breaking old jobs.\n",
    "* Prioritize **streaming & chunked** strategies early so you donâ€™t paint yourself into an in-memory corner.\n",
    "* Treat **security & privacy** as first-class: isolation by org, encryption, audit logs, retention.\n",
    "* Ship MVP quickly, then harden with observability, validation, and enterprise controls.\n",
    "\n",
    "If you want, I can turn any section above into **starter code** (repo skeleton with FastAPI, Celery, S3 uploads, recipe registry, and a Next.js upload + job UI) so you can run it locally with docker-compose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2db1a6-42f4-4acc-873a-db843da46e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00948bc2-a91e-4083-9a99-f326b4058918",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Great question ğŸ™Œ â€” when building a **data-cleaning web app** (with a backend in Python + frontend + worker system), itâ€™s very important to organize the code **into folders** so you (and others) can maintain it easily.\n",
    "\n",
    "Iâ€™ll show you a **clear folder structure** and explain what goes where in **simple language**.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“‚ Project Folder Breakdown\n",
    "\n",
    "```\n",
    "datacleaner-platform/\n",
    "â”‚\n",
    "â”œâ”€â”€ backend/                 # The engine room (Python FastAPI backend)\n",
    "â”‚   â”œâ”€â”€ app/\n",
    "â”‚   â”‚   â”œâ”€â”€ main.py          # Entry point of the backend (starts FastAPI server)\n",
    "â”‚   â”‚   â”œâ”€â”€ config.py        # Settings (database URL, storage keys, secrets)\n",
    "â”‚   â”‚   â”œâ”€â”€ models.py        # Database models (datasets, jobs, recipes, users)\n",
    "â”‚   â”‚   â”œâ”€â”€ schemas.py       # Data validation (Pydantic classes for API requests/responses)\n",
    "â”‚   â”‚   â”œâ”€â”€ routers/         # Different API endpoints\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py      # Login, signup, logout\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ datasets.py  # Upload, list, download datasets\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ recipes.py   # Create and manage cleaning recipes\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ jobs.py      # Start cleaning jobs and check progress\n",
    "â”‚   â”‚   â”œâ”€â”€ pipeline/        # The actual cleaning logic\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ ops.py       # All cleaning functions (handle missing, outliers, etc.)\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ runner.py    # Runs a full recipe step by step\n",
    "â”‚   â”‚   â”œâ”€â”€ tasks.py         # Celery tasks (workers run these)\n",
    "â”‚   â”‚   â”œâ”€â”€ db.py            # Database connection setup\n",
    "â”‚   â”‚   â””â”€â”€ utils.py         # Helper functions (logging, file handling)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ tests/               # Automated tests for backend\n",
    "â”‚   â”‚   â”œâ”€â”€ test_auth.py\n",
    "â”‚   â”‚   â”œâ”€â”€ test_jobs.py\n",
    "â”‚   â”‚   â””â”€â”€ test_pipeline.py\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ alembic/             # Database migrations\n",
    "â”‚   â”‚   â””â”€â”€ versions/        # Migration scripts (schema changes over time)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ Dockerfile           # Instructions to containerize backend\n",
    "â”‚   â”œâ”€â”€ requirements.txt     # Python libraries\n",
    "â”‚   â””â”€â”€ celery_worker.py     # Starts Celery worker for jobs\n",
    "â”‚\n",
    "â”œâ”€â”€ frontend/                # The waiter/front desk (React or Next.js)\n",
    "â”‚   â”œâ”€â”€ pages/               # Different web pages\n",
    "â”‚   â”‚   â”œâ”€â”€ index.tsx        # Home page (upload form)\n",
    "â”‚   â”‚   â”œâ”€â”€ jobs/[id].tsx    # Job progress page\n",
    "â”‚   â”‚   â”œâ”€â”€ recipes.tsx      # Recipe builder page\n",
    "â”‚   â”‚   â””â”€â”€ datasets.tsx     # List of uploaded datasets\n",
    "â”‚   â”œâ”€â”€ components/          # Reusable UI parts (buttons, progress bar, forms)\n",
    "â”‚   â”œâ”€â”€ hooks/               # Custom React hooks (e.g., useJobStatus)\n",
    "â”‚   â”œâ”€â”€ services/            # Functions to call the backend API\n",
    "â”‚   â”‚   â”œâ”€â”€ api.ts           # Generic API caller\n",
    "â”‚   â”‚   â”œâ”€â”€ datasets.ts      # Functions for datasets (upload, list, etc.)\n",
    "â”‚   â”‚   â””â”€â”€ jobs.ts          # Functions for job handling\n",
    "â”‚   â”œâ”€â”€ public/              # Static assets (logo, favicon)\n",
    "â”‚   â”œâ”€â”€ styles/              # CSS or Tailwind config\n",
    "â”‚   â”œâ”€â”€ package.json         # Dependencies for frontend\n",
    "â”‚   â””â”€â”€ tsconfig.json        # TypeScript config\n",
    "â”‚\n",
    "â”œâ”€â”€ infra/                   # Infrastructure (DevOps)\n",
    "â”‚   â”œâ”€â”€ docker-compose.yml   # Local setup (backend, frontend, db, redis, minio)\n",
    "â”‚   â”œâ”€â”€ k8s/                 # Kubernetes manifests (for cloud deployment)\n",
    "â”‚   â”œâ”€â”€ terraform/           # Scripts for setting up cloud resources (S3, DB, etc.)\n",
    "â”‚   â””â”€â”€ nginx.conf           # Reverse proxy config\n",
    "â”‚\n",
    "â”œâ”€â”€ docs/                    # Documentation for developers\n",
    "â”‚   â”œâ”€â”€ api.md               # API endpoint documentation\n",
    "â”‚   â”œâ”€â”€ setup.md             # How to run locally\n",
    "â”‚   â””â”€â”€ architecture.png     # System diagram\n",
    "â”‚\n",
    "â”œâ”€â”€ .env.example             # Example environment variables\n",
    "â”œâ”€â”€ README.md                # Overview of the project\n",
    "â””â”€â”€ .gitignore               # Ignore unwanted files in Git\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ—‚ï¸ What Each Part Means (Simple Explanation)\n",
    "\n",
    "| Folder/File               | Purpose (Simple Words)                                            |\n",
    "| ------------------------- | ----------------------------------------------------------------- |\n",
    "| **backend/app/main.py**   | Starts the backend server (the â€œengine roomâ€ manager).            |\n",
    "| **backend/app/routers/**  | Small â€œappsâ€ inside backend: login, datasets, recipes, jobs.      |\n",
    "| **backend/app/pipeline/** | Your Python data-cleaning logic (the heart).                      |\n",
    "| **backend/app/tasks.py**  | Jobs workers pick up when cleaning data.                          |\n",
    "| **backend/tests/**        | Files to test everything works correctly.                         |\n",
    "| **frontend/pages/**       | The actual web pages people see.                                  |\n",
    "| **frontend/components/**  | Small parts of the site like buttons, forms, progress bar.        |\n",
    "| **frontend/services/**    | Code to talk to backend API (send requests, get data).            |\n",
    "| **infra/**                | Deployment setup: how to run locally, on cloud, or in containers. |\n",
    "| **docs/**                 | Notes, diagrams, guides for developers.                           |\n",
    "| **README.md**             | The â€œfront pageâ€ of the project for anyone new.                   |\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ–¼ï¸ How It All Connects\n",
    "\n",
    "* **Frontend** â†’ users interact with web pages.\n",
    "* **Backend** â†’ API receives requests, stores files, creates jobs.\n",
    "* **Workers** (Celery) â†’ run Python cleaning code.\n",
    "* **Storage (S3/MinIO)** â†’ keeps files safe.\n",
    "* **Database (Postgres)** â†’ tracks users, datasets, jobs, recipes.\n",
    "* **Infra (Docker/K8s)** â†’ makes everything run together.\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘‰ This structure is **modular** (easy to find what you need), **scalable** (can add features without breaking), and **team-friendly** (frontend, backend, infra can be worked on separately).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to also **draw a tree diagram with arrows** (like a visual map of the folders and their roles) so itâ€™s easier to explain to non-coders?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43905e31-5b60-4f15-b816-7bd1f1d75cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
