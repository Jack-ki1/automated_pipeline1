{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216bd100-1b21-491b-b5a1-67a3d762cf83",
   "metadata": {},
   "source": [
    "## test_code4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c43d41d-9aa1-462c-8e5b-a448399a17ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Streamlit app saved as test_code4.py\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "#import webbrowser\n",
    "import time\n",
    "\n",
    "# Streamlit app code\n",
    "test_code4='''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import plotly.express as px\n",
    "\n",
    "# -------------------------------\n",
    "# STREAMLIT SETUP\n",
    "# -------------------------------\n",
    "st.set_page_config(page_title=\"Universal Data Pipeline\", layout=\"wide\")\n",
    "st.title(\"üìäüìâFINESE CROSS INDUSTRY ANALYSIS SYSTEMüìâ\")\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    ".info-card {\n",
    "    display: flex;\n",
    "    justify-content: space-around;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    ".card {\n",
    "    background-color: #f9f9f9;\n",
    "    padding: 1rem;\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 2px 2px 6px rgba(0,0,0,0.1);\n",
    "    width: 20%;\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Load Data\n",
    "# -------------------------------\n",
    "st.header(\"üìÅ Step 1: Upload or Fetch Your Data\")\n",
    "data_source = st.selectbox(\"Choose your data source\", [\"Upload File\", \"Paste JSON\", \"API Request\", \"Web Scraping\"])\n",
    "df = pd.DataFrame()\n",
    "\n",
    "if data_source == \"Upload File\":\n",
    "    uploaded_file = st.file_uploader(\"Upload CSV, Excel, or JSON file\", type=[\"csv\", \"xlsx\", \"json\"])\n",
    "    if uploaded_file:\n",
    "        if uploaded_file.name.endswith(\".csv\"):\n",
    "            df = pd.read_csv(uploaded_file)\n",
    "        elif uploaded_file.name.endswith(\".xlsx\"):\n",
    "            df = pd.read_excel(uploaded_file)\n",
    "        elif uploaded_file.name.endswith(\".json\"):\n",
    "            df = pd.read_json(uploaded_file)\n",
    "\n",
    "elif data_source == \"Paste JSON\":\n",
    "    json_data = st.text_area(\"Paste your JSON data\")\n",
    "    if json_data:\n",
    "        try:\n",
    "            df = pd.read_json(StringIO(json_data))\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error parsing JSON: {e}\")\n",
    "\n",
    "elif data_source == \"API Request\":\n",
    "    url = st.text_input(\"Enter the API endpoint URL\")\n",
    "    if url:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                df = pd.read_json(StringIO(response.text))\n",
    "                st.success(\"Data fetched successfully\")\n",
    "            else:\n",
    "                st.error(\"API request failed\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Request failed: {e}\")\n",
    "\n",
    "elif data_source == \"Web Scraping\":\n",
    "    page_url = st.text_input(\"Enter webpage URL to scrape tables\")\n",
    "    if page_url:\n",
    "        try:\n",
    "            html = requests.get(page_url).text\n",
    "            tables = pd.read_html(html)\n",
    "            table_index = st.number_input(\"Select table index\", 0, len(tables) - 1)\n",
    "            df = tables[int(table_index)]\n",
    "        except Exception as e:\n",
    "            st.error(f\"Web scraping failed: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Understand Data\n",
    "# -------------------------------\n",
    "if not df.empty:\n",
    "    st.header(\"üîç Step 2: Understand Your Data\")\n",
    "\n",
    "\n",
    "    num_rows, num_cols = df.shape\n",
    "    num_nulls = df.isnull().sum().sum()\n",
    "    num_duplicates = df.duplicated().sum()\n",
    "    memory_usage = df.memory_usage(deep=True).sum() / (1024**2)  # MB\n",
    "\n",
    "    # --- Summary cards ---\n",
    "    st.markdown(f\"\"\"\n",
    "    <div class='info-card'>\n",
    "        <div class='card'><strong>Total Rows</strong><br>{num_rows:,}</div>\n",
    "        <div class='card'><strong>Total Columns</strong><br>{num_cols}</div>\n",
    "        <div class='card'><strong>Total Nulls</strong><br>{num_nulls:,}</div>\n",
    "        <div class='card'><strong>Duplicate Rows</strong><br>{num_duplicates:,}</div>\n",
    "        <div class='card'><strong>Memory Usage</strong><br>{memory_usage:.2f} MB</div>\n",
    "    </div>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "    # --- Tabs for deeper insights ---\n",
    "    tab1, tab2, tab3, tab4 = st.tabs([\"üìë Preview\", \"‚ÑπÔ∏è Metadata\", \"üìä Stats\", \"üìâ Visual Insights\"])\n",
    "\n",
    "    # =====================\n",
    "    # TAB 1: Preview\n",
    "    # =====================\n",
    "    with tab1:\n",
    "        st.subheader(\"üëÄ Quick Preview\")\n",
    "        st.write(\"First 5 rows of the dataset:\")\n",
    "        st.dataframe(df.head())\n",
    "        st.write(\"Last 5 rows of the dataset:\")\n",
    "        st.dataframe(df.tail())\n",
    "\n",
    "    # =====================\n",
    "    # TAB 2: Metadata\n",
    "    # =====================\n",
    "    with tab2:\n",
    "        st.subheader(\"üßæ Column Metadata\")\n",
    "\n",
    "        # Show df.info() nicely\n",
    "        buffer = StringIO()\n",
    "        df.info(buf=buffer)\n",
    "        st.text(buffer.getvalue())\n",
    "\n",
    "        # Build metadata table\n",
    "        missing_counts = df.isnull().sum()\n",
    "        pct_missing = (missing_counts / len(df) * 100).round(2)  # ‚úÖ FIX: use Series.round(2), not round(ndarray, 2)\n",
    "\n",
    "        meta_df = pd.DataFrame({\n",
    "            \"Column\": df.columns,\n",
    "            \"Data Type\": df.dtypes.astype(str).values,\n",
    "            \"Missing Values\": missing_counts.values,\n",
    "            \"% Missing\": pct_missing.values,\n",
    "            \"Unique Values\": df.nunique().values,\n",
    "            \"Sample Value\": [df[col].dropna().iloc[0] if df[col].notna().any() else \"NaN\" for col in df.columns]\n",
    "        })\n",
    "        st.dataframe(meta_df)\n",
    "\n",
    "    # =====================\n",
    "    # TAB 3: Stats\n",
    "    # =====================\n",
    "    with tab3:\n",
    "        st.subheader(\"üìä Descriptive Statistics\")\n",
    "\n",
    "        # Numerical summary (guard for no numeric columns)\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(num_cols) > 0:\n",
    "            num_summary = df[num_cols].describe().T\n",
    "            num_summary[\"missing\"] = df[num_cols].isnull().sum()\n",
    "            st.write(\"### Numerical Columns Summary\")\n",
    "            st.dataframe(num_summary)\n",
    "        else:\n",
    "            st.info(\"No numerical columns detected.\")\n",
    "\n",
    "        # Categorical summary (guard for no categorical columns)\n",
    "        cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        if len(cat_cols) > 0:\n",
    "            st.write(\"### Categorical Columns Summary\")\n",
    "            cat_unique = df[cat_cols].nunique()\n",
    "            mode_df = df[cat_cols].mode(dropna=True)\n",
    "            most_freq = mode_df.iloc[0] if not mode_df.empty else pd.Series(index=cat_cols, dtype=object)\n",
    "            cat_summary = pd.DataFrame({\n",
    "                \"Unique Values\": cat_unique,\n",
    "                \"Most Frequent\": most_freq\n",
    "            })\n",
    "            st.dataframe(cat_summary)\n",
    "        else:\n",
    "            st.info(\"No categorical columns detected.\")\n",
    "\n",
    "        st.write(\"### Full Summary (All Types)\")\n",
    "        # df.describe(include=\"all\") can be empty if df has weird dtypes; guard it\n",
    "        try:\n",
    "            st.dataframe(df.describe(include=\"all\").T)\n",
    "        except Exception:\n",
    "            st.info(\"Full summary not available for this mix of dtypes.\")\n",
    "\n",
    "    # =====================\n",
    "    # TAB 4: Visual Insights\n",
    "    # =====================\n",
    "    with tab4:\n",
    "        st.subheader(\"üìâ Data Quality & Distribution Insights\")\n",
    "\n",
    "        # Missing values per column\n",
    "        missing_df = missing_counts.reset_index()\n",
    "        missing_df.columns = [\"Column\", \"Missing Count\"]\n",
    "        fig = px.bar(missing_df, x=\"Column\", y=\"Missing Count\", title=\"Missing Values per Column\")\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "        # Top numeric column distributions (up to 3)\n",
    "        top_numeric = list(num_cols)[:3]\n",
    "        if len(top_numeric) > 0:\n",
    "            st.write(\"### Distribution of Numeric Columns\")\n",
    "            for col in top_numeric:\n",
    "                fig = px.histogram(df, x=col, title=f\"Distribution of {col}\")\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "        # Top categorical column frequencies (up to 3)\n",
    "        top_cat = list(cat_cols)[:3]\n",
    "        if len(top_cat) > 0:\n",
    "            st.write(\"### Frequency of Categorical Columns\")\n",
    "            for col in top_cat:\n",
    "                vc = df[col].astype(str).value_counts(dropna=False).reset_index()\n",
    "                vc.columns = [col, \"Count\"]\n",
    "                fig = px.bar(vc.head(20), x=col, y=\"Count\", title=f\"Top Categories in {col}\")\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 3: Clean Data\n",
    "    # -------------------------------\n",
    "    st.header(\"üßπ Step 3: Clean Your Data\")\n",
    "\n",
    "    st.markdown(\"### Null Value Handling\")\n",
    "    st.write(f\"Total missing values: **{df.isnull().sum().sum()}** across {df.isnull().any().sum()} columns.\")\n",
    "\n",
    "    null_strategy = st.selectbox(\n",
    "        \"Choose how to handle missing values:\",\n",
    "        [\n",
    "            \"Do nothing\",\n",
    "            \"Drop rows with any nulls\",\n",
    "            \"Drop rows with all nulls\",\n",
    "            \"Fill with column mean (numeric only)\",\n",
    "            \"Fill with column median (numeric only)\",\n",
    "            \"Fill with column mode (all types)\",\n",
    "            \"Forward fill (propagate last valid value)\",\n",
    "            \"Backward fill (propagate next valid value)\",\n",
    "            \"Custom fill per column\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if null_strategy == \"Drop rows with any nulls\":\n",
    "        df = df.dropna(how=\"any\")\n",
    "    elif null_strategy == \"Drop rows with all nulls\":\n",
    "        df = df.dropna(how=\"all\")\n",
    "    elif null_strategy == \"Fill with column mean (numeric only)\":\n",
    "        for col in df.select_dtypes(include=[\"number\"]).columns:\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "    elif null_strategy == \"Fill with column median (numeric only)\":\n",
    "        for col in df.select_dtypes(include=[\"number\"]).columns:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    elif null_strategy == \"Fill with column mode (all types)\":\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().any():\n",
    "                try:\n",
    "                    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "                except Exception:\n",
    "                    pass\n",
    "    elif null_strategy == \"Forward fill (propagate last valid value)\":\n",
    "        df = df.ffill()\n",
    "    elif null_strategy == \"Backward fill (propagate next valid value)\":\n",
    "        df = df.bfill()\n",
    "    elif null_strategy == \"Custom fill per column\":\n",
    "        st.write(\"Specify replacement values for each column with missing data:\")\n",
    "        fill_values = {}\n",
    "        for col in df.columns[df.isnull().any()]:\n",
    "            replacement = st.text_input(f\"Fill value for column `{col}` (current nulls: {df[col].isnull().sum()})\")\n",
    "            if replacement != \"\":\n",
    "                fill_values[col] = replacement\n",
    "        if st.button(\"Apply Custom Fill\"):\n",
    "            for col, replacement in fill_values.items():\n",
    "                try:\n",
    "                    df[col] = df[col].fillna(type(df[col].dropna().iloc[0])(replacement))\n",
    "                except Exception:\n",
    "                    df[col] = df[col].fillna(replacement)\n",
    "            st.success(\"‚úÖ Custom fill applied.\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### Duplicate Handling\")\n",
    "    st.write(f\"Total duplicate rows detected: **{df.duplicated().sum()}**\")\n",
    "\n",
    "    dup_strategy = st.selectbox(\n",
    "        \"Choose how to handle duplicates:\",\n",
    "        [\n",
    "            \"Do nothing\",\n",
    "            \"Drop all duplicates (keep first)\",\n",
    "            \"Drop all duplicates (keep last)\",\n",
    "            \"Drop duplicates across selected subset of columns\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if dup_strategy == \"Drop all duplicates (keep first)\":\n",
    "        df = df.drop_duplicates(keep=\"first\")\n",
    "    elif dup_strategy == \"Drop all duplicates (keep last)\":\n",
    "        df = df.drop_duplicates(keep=\"last\")\n",
    "    elif dup_strategy == \"Drop duplicates across selected subset of columns\":\n",
    "        subset_cols = st.multiselect(\"Select columns to check duplicates on:\", df.columns)\n",
    "        if subset_cols:\n",
    "            df = df.drop_duplicates(subset=subset_cols, keep=\"first\")\n",
    "\n",
    "    st.success(\"‚úÖ Data cleaned.\")\n",
    "    st.dataframe(df.head())\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 3b: Data Type Checking\n",
    "    # -------------------------------\n",
    "    st.subheader(\"üõ† Step 3b: Check & Correct Column Data Types\")\n",
    "    st.write(\"The system will suggest data types based on heuristics. You can override them below:\")\n",
    "\n",
    "    # Function to auto-suggest column types\n",
    "    def suggest_dtype(series: pd.Series):\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            return \"int64\" if pd.api.types.is_integer_dtype(series) else \"float64\"\n",
    "        elif pd.api.types.is_datetime64_any_dtype(series):\n",
    "            return \"datetime64[ns]\"\n",
    "        elif pd.api.types.is_bool_dtype(series):\n",
    "            return \"bool\"\n",
    "        else:\n",
    "            # Try parsing as datetime\n",
    "            try:\n",
    "                pd.to_datetime(series.dropna().sample(min(50, len(series))), errors=\"raise\")\n",
    "                return \"datetime64[ns]\"\n",
    "            except Exception:\n",
    "                pass\n",
    "            # Try numeric\n",
    "            if series.dropna().astype(str).str.replace(\".\", \"\", 1).str.isnumeric().all():\n",
    "                return \"float64\"\n",
    "            # Try boolean\n",
    "            if series.dropna().astype(str).str.lower().isin([\"true\", \"false\", \"yes\", \"no\", \"0\", \"1\"]).all():\n",
    "                return \"bool\"\n",
    "            # Categorical suggestion\n",
    "            if series.nunique() < (0.05 * len(series)):\n",
    "                return \"category\"\n",
    "            return \"object\"\n",
    "\n",
    "    # Build selection UI\n",
    "    col_types = {}\n",
    "    for col in df.columns:\n",
    "        suggested_type = suggest_dtype(df[col])\n",
    "        current_type = str(df[col].dtype)\n",
    "        st.markdown(f\"**Column:** `{col}` | Detected: `{current_type}` | Suggested: `{suggested_type}`\")\n",
    "\n",
    "        new_type = st.selectbox(\n",
    "            f\"Select type for '{col}'\",\n",
    "            [\"int64\", \"float64\", \"object\", \"datetime64[ns]\", \"bool\", \"category\"],\n",
    "            index=[\"int64\", \"float64\", \"object\", \"datetime64[ns]\", \"bool\", \"category\"].index(suggested_type)\n",
    "            if suggested_type in [\"int64\", \"float64\", \"object\", \"datetime64[ns]\", \"bool\", \"category\"] else 2\n",
    "        )\n",
    "        col_types[col] = new_type\n",
    "\n",
    "    if st.button(\"Apply Type Conversions\"):\n",
    "        for col, t in col_types.items():\n",
    "            try:\n",
    "                if t == \"datetime64[ns]\":\n",
    "                    df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "                elif t == \"bool\":\n",
    "                    df[col] = df[col].astype(str).str.lower().map(\n",
    "                        {\"true\": True, \"false\": False, \"yes\": True, \"no\": False, \"1\": True, \"0\": False}\n",
    "                    ).astype(\"boolean\")\n",
    "                elif t == \"category\":\n",
    "                    df[col] = df[col].astype(\"category\")\n",
    "                else:\n",
    "                    df[col] = df[col].astype(t, errors=\"ignore\")\n",
    "            except Exception as e:\n",
    "                st.warning(f\"‚ö†Ô∏è Could not convert {col} to {t}: {e}\")\n",
    "        st.success(\"‚úÖ Data type conversions applied.\")\n",
    "        st.dataframe(df.dtypes)\n",
    "\n",
    "    # Download cleaned data\n",
    "    st.download_button(\"üì• Download Cleaned Data\", df.to_csv(index=False), \"cleaned_data.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: Visualize Data\n",
    "# -------------------------------\n",
    "    st.markdown(\"### üìä Data Visualization\")\n",
    "\n",
    "    num_charts = st.number_input(\"How many plots would you like to generate?\", min_value=1, step=1)\n",
    "    chart_configs = []\n",
    "\n",
    "    chart_types = [\"Bar\", \"Line\", \"Scatter\", \"Histogram\", \"Box\"]\n",
    "\n",
    "    for i in range(int(num_charts)):\n",
    "        st.markdown(f\"**Chart {i+1} Configuration**\")\n",
    "        chart_type = st.selectbox(f\"Select chart type for Chart {i+1}\", chart_types, key=f\"type_{i}\")\n",
    "        x_axis = st.selectbox(f\"Select x-axis for Chart {i+1}\", [\"None\"] + list(df.columns), key=f\"x_{i}\")\n",
    "        y_axis = st.selectbox(f\"Select y-axis for Chart {i+1}\", [\"None\"] + list(df.columns), key=f\"y_{i}\")\n",
    "        color_axis = st.selectbox(f\"Optional: Select column for color (Chart {i+1})\", [\"None\"] + list(df.columns), key=f\"color_{i}\")\n",
    "        chart_configs.append({\"type\": chart_type, \"x\": x_axis, \"y\": y_axis, \"color\": color_axis})\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### üìà Your Charts\")\n",
    "\n",
    "    # Display charts 2 per row\n",
    "    for i in range(0, len(chart_configs), 2):\n",
    "        cols = st.columns(2)\n",
    "        for j in range(2):\n",
    "            if i + j < len(chart_configs):\n",
    "                config = chart_configs[i + j]\n",
    "                with cols[j]:\n",
    "                    if config['x'] != \"None\" and config['y'] != \"None\":\n",
    "                        if config['type'] == \"Bar\":\n",
    "                            fig = px.bar(df, x=config['x'], y=config['y'], color=None if config['color'] == \"None\" else config['color'])\n",
    "                        elif config['type'] == \"Line\":\n",
    "                            fig = px.line(df, x=config['x'], y=config['y'], color=None if config['color'] == \"None\" else config['color'])\n",
    "                        elif config['type'] == \"Scatter\":\n",
    "                            fig = px.scatter(df, x=config['x'], y=config['y'], color=None if config['color'] == \"None\" else config['color'])\n",
    "                        elif config['type'] == \"Histogram\":\n",
    "                            fig = px.histogram(df, x=config['x'], y=config['y'], color=None if config['color'] == \"None\" else config['color'])\n",
    "                        elif config['type'] == \"Box\":\n",
    "                            fig = px.box(df, x=config['x'], y=config['y'], color=None if config['color'] == \"None\" else config['color'])\n",
    "                        st.plotly_chart(fig, use_container_width=True)\n",
    "                    else:\n",
    "                        st.warning(f\"Chart {i+j+1} skipped: x or y axis not specified.\")\n",
    "else:\n",
    "    st.warning(\"Please load your data to proceed.\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 5: Download Data Report\n",
    "# -------------------------------\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Table, TableStyle, Spacer\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "import tempfile\n",
    "\n",
    "st.header(\"üì• Step 5: Download Data Report\")\n",
    "\n",
    "if not df.empty:\n",
    "    if st.button(\"Generate PDF Report\"):\n",
    "        # Create temporary file\n",
    "        tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\")\n",
    "        doc = SimpleDocTemplate(tmp_file.name, pagesize=letter)\n",
    "        styles = getSampleStyleSheet()\n",
    "        elements = []\n",
    "\n",
    "        # Title\n",
    "        elements.append(Paragraph(\"üìä Data Report\", styles[\"Title\"]))\n",
    "        elements.append(Spacer(1, 12))\n",
    "\n",
    "        # --- Dataset Summary ---\n",
    "        elements.append(Paragraph(\"Dataset Summary\", styles[\"Heading2\"]))\n",
    "        summary_data = [\n",
    "            [\"Rows\", df.shape[0]],\n",
    "            [\"Columns\", df.shape[1]],\n",
    "            [\"Total Nulls\", int(df.isnull().sum().sum())],\n",
    "            [\"Duplicate Rows\", int(df.duplicated().sum())],\n",
    "        ]\n",
    "        summary_table = Table(summary_data, colWidths=[200, 200])\n",
    "        summary_table.setStyle(TableStyle([\n",
    "            (\"BACKGROUND\", (0,0), (-1,0), colors.grey),\n",
    "            (\"TEXTCOLOR\", (0,0), (-1,0), colors.whitesmoke),\n",
    "            (\"ALIGN\", (0,0), (-1,-1), \"CENTER\"),\n",
    "            (\"GRID\", (0,0), (-1,-1), 1, colors.black),\n",
    "        ]))\n",
    "        elements.append(summary_table)\n",
    "        elements.append(Spacer(1, 12))\n",
    "\n",
    "        # --- Column Metadata ---\n",
    "        elements.append(Paragraph(\"Column Metadata\", styles[\"Heading2\"]))\n",
    "        meta_data = [[\"Column\", \"Dtype\", \"Missing %\", \"Unique Values\"]]\n",
    "        for col in df.columns:\n",
    "            missing_pct = round(df[col].isnull().sum() / len(df) * 100, 2)\n",
    "            meta_data.append([col, str(df[col].dtype), f\"{missing_pct}%\", df[col].nunique()])\n",
    "        meta_table = Table(meta_data, colWidths=[150, 100, 100, 100])\n",
    "        meta_table.setStyle(TableStyle([\n",
    "            (\"BACKGROUND\", (0,0), (-1,0), colors.grey),\n",
    "            (\"TEXTCOLOR\", (0,0), (-1,0), colors.whitesmoke),\n",
    "            (\"ALIGN\", (0,0), (-1,-1), \"CENTER\"),\n",
    "            (\"GRID\", (0,0), (-1,-1), 1, colors.black),\n",
    "        ]))\n",
    "        elements.append(meta_table)\n",
    "        elements.append(Spacer(1, 12))\n",
    "\n",
    "        # --- Numerical Summary ---\n",
    "        elements.append(Paragraph(\"Numerical Data Summary\", styles[\"Heading2\"]))\n",
    "        desc = df.describe(include=\"all\").fillna(\"\").astype(str).reset_index()\n",
    "        desc_data = [desc.columns.tolist()] + desc.values.tolist()\n",
    "        desc_table = Table(desc_data, colWidths=[100]*len(desc.columns))\n",
    "        desc_table.setStyle(TableStyle([\n",
    "            (\"BACKGROUND\", (0,0), (-1,0), colors.grey),\n",
    "            (\"TEXTCOLOR\", (0,0), (-1,0), colors.whitesmoke),\n",
    "            (\"ALIGN\", (0,0), (-1,-1), \"CENTER\"),\n",
    "            (\"GRID\", (0,0), (-1,-1), 0.5, colors.black),\n",
    "        ]))\n",
    "        elements.append(desc_table)\n",
    "\n",
    "        # Build PDF\n",
    "        doc.build(elements)\n",
    "\n",
    "        # Read PDF and make it downloadable\n",
    "        with open(tmp_file.name, \"rb\") as f:\n",
    "            st.download_button(\n",
    "                label=\"‚¨áÔ∏è Download Data Report (PDF)\",\n",
    "                data=f,\n",
    "                file_name=\"data_report.pdf\",\n",
    "                mime=\"application/pdf\"\n",
    "            )\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Save code to file\n",
    "#with open(\"test_code4.py\", \"w\") as f:\n",
    "with open(\"test_code4.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    f.write(test_code4)\n",
    "\n",
    "print(\"‚úÖ Streamlit app saved as test_code4.py\")\n",
    "\n",
    "# Run the app\n",
    "process = subprocess.Popen([\"streamlit\", \"run\", \"test_code4.py\"])\n",
    "\n",
    "# Wait for server to start\n",
    "time.sleep(5)\n",
    "\n",
    "# Open in browser\n",
    "#webbrowser.open(\"http://localhost:8501\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7499e8e2-e75e-4623-acd6-bc60efea2846",
   "metadata": {},
   "source": [
    "prompts used to advance code several times:\n",
    "you are a python data analyst with a lot of experience now you were coming up with a streamlit code but you now need gpt to help you finsh it and correct the following:\n",
    "now let us look into step 3b further, i need you to use your knowledge and research to modify step three such that is fully ready to handel any challenge inregards to data types that can occur in a dataset:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
